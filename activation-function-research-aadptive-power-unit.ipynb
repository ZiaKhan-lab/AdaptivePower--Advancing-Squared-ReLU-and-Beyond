{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Activation Function Research & Benchmarks\n\nThis notebook implements and compares:\n- **Standard**: ReLU, LeakyReLU, PReLU,  \n- **Squared-ReLU Variants**: Soft (SoftSqReLU), Exponential (ExpSqReLU)  \n- **Advanced**: Adaptive Power Unit (APU), Neuron Resurrection Activation (NRA)  \n\n**Datasets**: MNIST, EMNIST (letters), CIFAR-10  \n**Architectures**: SimpleCNN, ResNet-18, Transformer, GAN  \n**Metrics**: accuracy, loss, grad-norm, dead-neuron rate, GAN losses, runtime  \nExport results for theoretical analysis and paper writing.","metadata":{}},{"cell_type":"markdown","source":"## Adaptive Power Unit (APU)\n\nBlock formula:\n\n$$\np(z) = p_0 + w_p \\,\\tanh\\bigl(w_z\\,z + b_z\\bigr),\n\\quad\nf(z) = \\mathrm{sign}(z)\\,\\lvert z\\rvert^{p(z)}.\n$$\n\n- **Differentiability**  \n  The function is smooth for all $z \\neq 0$ (we clamp $\\lvert z\\rvert\\ge\\epsilon$ if you want to be extra safe near zero).\n\n- **Lipschitz bound**  \n  You can show\n  $$\n    f'(z)\n    = p(z)\\,\\lvert z\\rvert^{p(z)-1}\n      \\;+\\;\n      \\frac{\\partial p}{\\partial z}\\,\\lvert z\\rvert^{p(z)}\\ln\\lvert z\\rvert,\n  $$\n  and then bound each term over your training range.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport os\nfrom contextlib import redirect_stdout\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:02.513763Z","iopub.execute_input":"2025-04-28T17:00:02.514364Z","iopub.status.idle":"2025-04-28T17:00:08.976421Z","shell.execute_reply.started":"2025-04-28T17:00:02.514339Z","shell.execute_reply":"2025-04-28T17:00:08.975834Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:08.977514Z","iopub.execute_input":"2025-04-28T17:00:08.977867Z","iopub.status.idle":"2025-04-28T17:00:09.061470Z","shell.execute_reply.started":"2025-04-28T17:00:08.977844Z","shell.execute_reply":"2025-04-28T17:00:09.060239Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### 1. ACTIVATION FUNCTIONS","metadata":{}},{"cell_type":"code","source":"class ParametricReLU(nn.Module):\n    def __init__(self, init_alpha=0.1, learnable=True):\n        super().__init__()\n        self.alpha = nn.Parameter(torch.tensor(init_alpha)) if learnable else init_alpha\n        \n    def forward(self, x):\n        return torch.where(x > 0, x, self.alpha * x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:17.630498Z","iopub.execute_input":"2025-04-28T17:00:17.630780Z","iopub.status.idle":"2025-04-28T17:00:17.635564Z","shell.execute_reply.started":"2025-04-28T17:00:17.630758Z","shell.execute_reply":"2025-04-28T17:00:17.634771Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class SoftReLU(nn.Module):\n    def __init__(self, beta=1.0):\n        super().__init__()\n        self.beta = beta\n        \n    def forward(self, x):\n        return x * torch.sigmoid(self.beta * x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:20.742027Z","iopub.execute_input":"2025-04-28T17:00:20.742325Z","iopub.status.idle":"2025-04-28T17:00:20.746818Z","shell.execute_reply.started":"2025-04-28T17:00:20.742301Z","shell.execute_reply":"2025-04-28T17:00:20.746175Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class ExponentialSquaredReLU(nn.Module):\n    def __init__(self, alpha=0.1, gamma=1.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, x):\n        pos_part = torch.where(x > 0, x, torch.zeros_like(x))\n        neg_part = torch.where(x < 0, self.alpha * (torch.exp(-self.gamma * x*x) - 1), torch.zeros_like(x))\n        return pos_part + neg_part","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:23.102424Z","iopub.execute_input":"2025-04-28T17:00:23.102878Z","iopub.status.idle":"2025-04-28T17:00:23.107470Z","shell.execute_reply.started":"2025-04-28T17:00:23.102853Z","shell.execute_reply":"2025-04-28T17:00:23.106765Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class AdaptivePowerUnit(nn.Module):\n    def __init__(self, p0=1.0):\n        super().__init__()\n        self.p0 = p0\n        self.w_p = nn.Parameter(torch.randn(1) * 0.1)\n        self.w_z = nn.Parameter(torch.randn(1) * 0.1)\n        self.b_z = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, z):\n        p_z = self.p0 + self.w_p * torch.tanh(self.w_z * z + self.b_z)\n        # small epsilon to avoid zero division\n        eps = 1e-6\n        return torch.sign(z) * torch.abs(z).clamp(min=eps)**p_z","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:25.002853Z","iopub.execute_input":"2025-04-28T17:00:25.003138Z","iopub.status.idle":"2025-04-28T17:00:25.008827Z","shell.execute_reply.started":"2025-04-28T17:00:25.003117Z","shell.execute_reply":"2025-04-28T17:00:25.007948Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Dictionary of all activation functions for testing\nactivations = {\n    'ReLU': nn.ReLU,\n    'LeakyReLU': lambda: nn.LeakyReLU(0.01),\n    'PReLU': lambda: ParametricReLU(0.1, True),\n    'SoftReLU': lambda: SoftReLU(1.0),\n    'ESReLU': lambda: ExponentialSquaredReLU(0.1, 1.0),\n    'APU': AdaptivePowerUnit\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:27.064078Z","iopub.execute_input":"2025-04-28T17:00:27.064583Z","iopub.status.idle":"2025-04-28T17:00:27.069496Z","shell.execute_reply.started":"2025-04-28T17:00:27.064560Z","shell.execute_reply":"2025-04-28T17:00:27.068700Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### 2. MODELS","metadata":{}},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self, act_name, in_channels=1, num_classes=10):\n        super().__init__()\n        self.in_channels = in_channels\n        self.conv1 = nn.Conv2d(in_channels, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        \n        # Determine feature size based on input channels (MNIST vs CIFAR)\n        feature_size = 9216 if in_channels == 1 else 12544\n        \n        self.fc1 = nn.Linear(feature_size, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        \n        # instantiate activations per layer\n        self.a1 = activations[act_name]() if act_name != 'NRA' else activations['NRA'](32)\n        self.a2 = activations[act_name]() if act_name != 'NRA' else activations['NRA'](64)\n        self.a3 = activations[act_name]() if act_name != 'NRA' else activations['NRA'](128)\n        \n    def forward(self, x):\n        x = self.a1(self.conv1(x))\n        x = self.a2(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = self.a3(self.fc1(x))\n        return self.fc2(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:29.013514Z","iopub.execute_input":"2025-04-28T17:00:29.013774Z","iopub.status.idle":"2025-04-28T17:00:29.020511Z","shell.execute_reply.started":"2025-04-28T17:00:29.013753Z","shell.execute_reply":"2025-04-28T17:00:29.019650Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Basic ResNet block with configurable activation\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, act_name='ReLU'):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                               stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.act1 = activations[act_name]() if act_name != 'NRA' else activations['NRA'](out_channels)\n        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.act2 = activations[act_name]() if act_name != 'NRA' else activations['NRA'](out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        out = self.act1(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = self.act2(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, act_name, num_blocks=[2, 2, 2, 2], num_classes=10, in_channels=3):\n        super().__init__()\n        self.in_channels = in_channels\n        self.in_planes = 64\n        \n        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.act1 = activations[act_name]() if act_name != 'NRA' else activations['NRA'](64)\n        \n        self.layer1 = self._make_layer(64, num_blocks[0], stride=1, act_name=act_name)\n        self.layer2 = self._make_layer(128, num_blocks[1], stride=2, act_name=act_name)\n        self.layer3 = self._make_layer(256, num_blocks[2], stride=2, act_name=act_name)\n        self.layer4 = self._make_layer(512, num_blocks[3], stride=2, act_name=act_name)\n        \n        self.linear = nn.Linear(512, num_classes)\n\n    def _make_layer(self, planes, num_blocks, stride, act_name):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(ResidualBlock(self.in_planes, planes, stride, act_name))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.act1(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:31.667028Z","iopub.execute_input":"2025-04-28T17:00:31.667287Z","iopub.status.idle":"2025-04-28T17:00:31.678935Z","shell.execute_reply.started":"2025-04-28T17:00:31.667268Z","shell.execute_reply":"2025-04-28T17:00:31.678371Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Simple Transformer Block with configurable activation\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_dim, act_name='ReLU'):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, ff_dim),\n            activations[act_name]() if act_name != 'NRA' else activations['NRA'](ff_dim),\n            nn.Linear(ff_dim, embed_dim)\n        )\n        \n    def forward(self, x):\n        # Self-attention with residual connection\n        attn_output, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_output)\n        \n        # Feed-forward with residual connection\n        ff_output = self.ff(x)\n        x = self.norm2(x + ff_output)\n        \n        return x\n\n\nclass SimpleTransformer(nn.Module):\n    def __init__(self, act_name, input_dim=28*28, embed_dim=128, num_heads=4, \n                 ff_dim=512, num_blocks=2, num_classes=10):\n        super().__init__()\n        self.embedding = nn.Linear(input_dim, embed_dim)\n        self.pos_encoding = nn.Parameter(torch.randn(1, embed_dim))\n        \n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, ff_dim, act_name) \n            for _ in range(num_blocks)\n        ])\n        \n        self.classifier = nn.Linear(embed_dim, num_classes)\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        \n        # Flatten the input\n        x = x.view(batch_size, -1)\n        \n        # Linear embedding + positional encoding\n        x = self.embedding(x)\n        x = x + self.pos_encoding\n        \n        # Add sequence dimension (batch, seq_len=1, embed_dim)\n        x = x.unsqueeze(1)\n        \n        # Pass through transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Use the output of the last token for classification\n        x = x.squeeze(1)\n        \n        # Classification layer\n        return self.classifier(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:35.285781Z","iopub.execute_input":"2025-04-28T17:00:35.286507Z","iopub.status.idle":"2025-04-28T17:00:35.298555Z","shell.execute_reply.started":"2025-04-28T17:00:35.286479Z","shell.execute_reply":"2025-04-28T17:00:35.297817Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Simple GAN with configurable activation\nclass Generator(nn.Module):\n    def __init__(self, latent_dim=100, act_name='ReLU', out_channels=1):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.out_channels = out_channels\n        \n        self.model = nn.Sequential(\n            # Input: latent_dim\n            nn.Linear(latent_dim, 128),\n            activations[act_name]() if act_name != 'NRA' else activations['NRA'](128),\n            \n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            activations[act_name]() if act_name != 'NRA' else activations['NRA'](256),\n            \n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            activations[act_name]() if act_name != 'NRA' else activations['NRA'](512),\n            \n            nn.Linear(512, 784 * out_channels),\n            nn.Tanh()\n        )\n        \n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.size(0), self.out_channels, 28, 28)\n        return img\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, act_name='ReLU', in_channels=1):\n        super().__init__()\n        \n        self.model = nn.Sequential(\n            # Input: in_channels x 28 x 28\n            nn.Flatten(),\n            nn.Linear(784 * in_channels, 512),\n            activations[act_name]() if act_name != 'NRA' else activations['NRA'](512),\n            \n            nn.Linear(512, 256),\n            activations[act_name]() if act_name != 'NRA' else activations['NRA'](256),\n            \n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, img):\n        validity = self.model(img)\n        return validity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:38.580385Z","iopub.execute_input":"2025-04-28T17:00:38.581137Z","iopub.status.idle":"2025-04-28T17:00:38.591594Z","shell.execute_reply.started":"2025-04-28T17:00:38.581104Z","shell.execute_reply":"2025-04-28T17:00:38.590734Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### 3. TRAINING AND EVALUATION","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss, correct, total, grad_norm = 0, 0, 0, 0\n    \n    for imgs, labels in tqdm(loader, desc=\"Training\", leave=False):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        \n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # Calculate gradient norm\n        grad_norm_batch = sum(p.grad.norm(2).item() for p in model.parameters() if p.grad is not None)\n        grad_norm += grad_norm_batch / len(list(model.parameters()))\n        \n        optimizer.step()\n        \n        total_loss += loss.item() * imgs.size(0)\n        \n        # Calculate accuracy\n        preds = outputs.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n    \n    return total_loss / total, correct / total * 100, grad_norm / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:49.360768Z","iopub.execute_input":"2025-04-28T17:00:49.361442Z","iopub.status.idle":"2025-04-28T17:00:49.367312Z","shell.execute_reply.started":"2025-04-28T17:00:49.361417Z","shell.execute_reply":"2025-04-28T17:00:49.366452Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def evaluate_model(model, loader, device, criterion=None):\n    model.eval()\n    correct, total = 0, 0\n    test_loss = 0\n    \n    with torch.no_grad():\n        for imgs, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n            imgs, labels = imgs.to(device), labels.to(device)\n            \n            outputs = model(imgs)\n            preds = outputs.argmax(1)\n            \n            if criterion:\n                loss = criterion(outputs, labels)\n                test_loss += loss.item() * imgs.size(0)\n                \n            correct += (preds == labels).sum().item()\n            total += imgs.size(0)\n    \n    if criterion:\n        return correct / total * 100, test_loss / total\n    return correct / total * 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:53.556326Z","iopub.execute_input":"2025-04-28T17:00:53.556556Z","iopub.status.idle":"2025-04-28T17:00:53.561770Z","shell.execute_reply.started":"2025-04-28T17:00:53.556540Z","shell.execute_reply":"2025-04-28T17:00:53.561128Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_gan_epoch(gen, disc, g_opt, d_opt, loader, latent_dim, device):\n    gen.train()\n    disc.train()\n    \n    d_losses, g_losses = [], []\n    real_scores, fake_scores = [], []\n    \n    for real_imgs, _ in loader:\n        batch_size = real_imgs.size(0)\n        real_imgs = real_imgs.to(device)\n        \n        # Train Discriminator\n        d_opt.zero_grad()\n        \n        # Real images\n        real_validity = disc(real_imgs)\n        real_score = real_validity.mean().item()\n        real_scores.append(real_score)\n        \n        # Fake images\n        z = torch.randn(batch_size, latent_dim).to(device)\n        fake_imgs = gen(z)\n        fake_validity = disc(fake_imgs.detach())\n        fake_score = fake_validity.mean().item()\n        fake_scores.append(fake_score)\n        \n        # Discriminator loss\n        d_real_loss = F.binary_cross_entropy(real_validity, \n                                             torch.ones(batch_size, 1).to(device))\n        d_fake_loss = F.binary_cross_entropy(fake_validity, \n                                             torch.zeros(batch_size, 1).to(device))\n        d_loss = (d_real_loss + d_fake_loss) / 2\n        d_losses.append(d_loss.item())\n        \n        d_loss.backward()\n        d_opt.step()\n        \n        # Train Generator\n        g_opt.zero_grad()\n        \n        # Generate new fake images\n        z = torch.randn(batch_size, latent_dim).to(device)\n        fake_imgs = gen(z)\n        fake_validity = disc(fake_imgs)\n        \n        # Generator loss\n        g_loss = F.binary_cross_entropy(fake_validity, torch.ones(batch_size, 1).to(device))\n        g_losses.append(g_loss.item())\n        \n        g_loss.backward()\n        g_opt.step()\n    \n    return {\n        'd_loss': sum(d_losses) / len(d_losses),\n        'g_loss': sum(g_losses) / len(g_losses),\n        'real_score': sum(real_scores) / len(real_scores),\n        'fake_score': sum(fake_scores) / len(fake_scores)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:00.086929Z","iopub.execute_input":"2025-04-28T17:01:00.087267Z","iopub.status.idle":"2025-04-28T17:01:00.094548Z","shell.execute_reply.started":"2025-04-28T17:01:00.087246Z","shell.execute_reply":"2025-04-28T17:01:00.093941Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### 4. DATASETS","metadata":{}},{"cell_type":"code","source":"def get_dataset_loaders(dataset_name='MNIST', batch_size=64):\n    # Define a context manager to suppress stdout\n    with open(os.devnull, 'w') as devnull:\n        with redirect_stdout(devnull):\n            if dataset_name == 'MNIST':\n                transform = transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.1307,), (0.3081,))\n                ])\n                train_ds = torchvision.datasets.MNIST('data/', train=True, download=True, transform=transform)\n                test_ds = torchvision.datasets.MNIST('data/', train=False, download=True, transform=transform)\n                in_channels, num_classes = 1, 10\n                img_size = 28\n                \n            elif dataset_name == 'CIFAR10':\n                transform_train = transforms.Compose([\n                    transforms.RandomCrop(32, padding=4),\n                    transforms.RandomHorizontalFlip(),\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n                ])\n                transform_test = transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n                ])\n                train_ds = torchvision.datasets.CIFAR10('data/', train=True, download=True, transform=transform_train)\n                test_ds = torchvision.datasets.CIFAR10('data/', train=False, download=True, transform=transform_test)\n                in_channels, num_classes = 3, 10\n                img_size = 32\n                \n            elif dataset_name == 'EMNIST':\n                transform = transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.1307,), (0.3081,))\n                ])\n                train_ds = torchvision.datasets.EMNIST('data/', split='balanced', train=True, download=True, transform=transform)\n                test_ds = torchvision.datasets.EMNIST('data/', split='balanced', train=False, download=True, transform=transform)\n                in_channels, num_classes = 1, 47  # EMNIST balanced has 47 classes\n                img_size = 28\n                \n            else:\n                raise ValueError(f\"Dataset {dataset_name} not implemented\")\n    \n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_ds, batch_size=batch_size*2, shuffle=False)\n    \n    return train_loader, test_loader, in_channels, num_classes, img_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:04.726321Z","iopub.execute_input":"2025-04-28T17:01:04.726929Z","iopub.status.idle":"2025-04-28T17:01:04.734715Z","shell.execute_reply.started":"2025-04-28T17:01:04.726908Z","shell.execute_reply":"2025-04-28T17:01:04.733884Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### 5. BENCHMARK EXECUTION","metadata":{}},{"cell_type":"code","source":"def run_activation_benchmark(model_name, dataset_name, act_names, epochs=10, lr=0.001):\n    print(f\"\\n===== Running {model_name} on {dataset_name} =====\")\n    \n    # Get dataset with the updated function call to get img_size\n    train_loader, test_loader, in_channels, num_classes, img_size = get_dataset_loaders(dataset_name)\n    \n    # Initialize results dictionary\n    results = {name: {'train_loss': [], 'train_acc': [], 'test_acc': [], 'grad_norm': [], 'time': []}\n              for name in act_names}\n    \n    for act_name in act_names:\n        print(f\"\\n### Activation: {act_name}\")\n        \n        # Initialize model based on model_name\n        if model_name == 'SimpleCNN':\n            model = SimpleCNN(act_name, in_channels=in_channels, num_classes=num_classes).to(device)\n        elif model_name == 'ResNet':\n            model = ResNet(act_name, num_blocks=[2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels).to(device)\n        elif model_name == 'Transformer':\n            # Use the correct image size for calculating input dimensions\n            input_dim = in_channels * img_size * img_size\n            model = SimpleTransformer(act_name, input_dim=input_dim, num_classes=num_classes).to(device)\n        else:\n            raise ValueError(f\"Model {model_name} not implemented\")\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n        \n        for epoch in range(1, epochs+1):\n            epoch_start_time = time.time()\n            \n            # Train for one epoch\n            train_loss, train_acc, grad_norm = train_epoch(model, train_loader, optimizer, criterion, device)\n            \n            # Evaluate\n            test_acc = evaluate_model(model, test_loader, device)\n            \n            # Record time\n            epoch_time = time.time() - epoch_start_time\n            \n            # Save results\n            res = results[act_name]\n            res['train_loss'].append(train_loss)\n            res['train_acc'].append(train_acc)\n            res['test_acc'].append(test_acc)\n            res['grad_norm'].append(grad_norm)\n            res['time'].append(epoch_time)\n            \n            print(f\"Epoch {epoch}/{epochs}: loss={train_loss:.4f}, train_acc={train_acc:.1f}%, \"\n                  f\"test_acc={test_acc:.1f}%, grad_norm={grad_norm:.2f}, time={epoch_time:.2f}s\")\n    \n    # Plot results\n    plot_benchmark_results(results, model_name, dataset_name)\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:10.226460Z","iopub.execute_input":"2025-04-28T17:01:10.227294Z","iopub.status.idle":"2025-04-28T17:01:10.235486Z","shell.execute_reply.started":"2025-04-28T17:01:10.227270Z","shell.execute_reply":"2025-04-28T17:01:10.234795Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def run_gan_benchmark(dataset_name, act_names, epochs=10, lr=0.0002, latent_dim=100):\n    print(f\"\\n===== Running GAN on {dataset_name} =====\")\n    \n    # Get dataset\n    train_loader, _, in_channels, _, _ = get_dataset_loaders(dataset_name)\n    \n    # Initialize results dictionary\n    results = {name: {'d_loss': [], 'g_loss': [], 'real_score': [], 'fake_score': [], 'time': []}\n              for name in act_names}\n    \n    for act_name in act_names:\n        print(f\"\\n### Activation: {act_name}\")\n        \n        # Initialize Generator and Discriminator\n        generator = Generator(latent_dim=latent_dim, act_name=act_name, out_channels=in_channels).to(device)\n        discriminator = Discriminator(act_name=act_name, in_channels=in_channels).to(device)\n        \n        # Optimizers\n        g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n        d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n        \n        for epoch in range(1, epochs+1):\n            epoch_start_time = time.time()\n            \n            # Train GAN for one epoch\n            gan_metrics = train_gan_epoch(generator, discriminator, g_optimizer, d_optimizer, \n                                         train_loader, latent_dim, device)\n            \n            # Record time\n            epoch_time = time.time() - epoch_start_time\n            \n            # Save results\n            res = results[act_name]\n            for k, v in gan_metrics.items():\n                res[k].append(v)\n            res['time'].append(epoch_time)\n            \n            print(f\"Epoch {epoch}/{epochs}: d_loss={gan_metrics['d_loss']:.4f}, \"\n                  f\"g_loss={gan_metrics['g_loss']:.4f}, real_score={gan_metrics['real_score']:.4f}, \"\n                  f\"fake_score={gan_metrics['fake_score']:.4f}, time={epoch_time:.2f}s\")\n            \n            # Generate and save sample images every few epochs\n            if epoch % 5 == 0:\n                save_gan_samples(generator, act_name, epoch, latent_dim, in_channels, device)\n    \n    # Plot results\n    plot_gan_results(results, dataset_name)\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:23.804917Z","iopub.execute_input":"2025-04-28T17:01:23.805220Z","iopub.status.idle":"2025-04-28T17:01:23.813092Z","shell.execute_reply.started":"2025-04-28T17:01:23.805199Z","shell.execute_reply":"2025-04-28T17:01:23.812244Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def save_gan_samples(generator, act_name, epoch, latent_dim, channels, device, n_samples=16):\n    \"\"\"Generate and save sample images from the generator\"\"\"\n    generator.eval()\n    with torch.no_grad():\n        z = torch.randn(n_samples, latent_dim).to(device)\n        gen_imgs = generator(z).cpu()\n    \n    # Convert to matplotlib-compatible format and rescale to [0,1]\n    imgs = gen_imgs.detach().numpy()\n    imgs = (imgs + 1) / 2.0  # Scale from [-1,1] to [0,1]\n    \n    # Create directory if it doesn't exist\n    os.makedirs(f\"gan_samples/{act_name}\", exist_ok=True)\n    \n    # Plot and save\n    fig, axs = plt.subplots(int(np.sqrt(n_samples)), int(np.sqrt(n_samples)))\n    cnt = 0\n    for i in range(int(np.sqrt(n_samples))):\n        for j in range(int(np.sqrt(n_samples))):\n            if channels == 1:\n                axs[i, j].imshow(imgs[cnt, 0], cmap='gray')\n            else:\n                axs[i, j].imshow(np.transpose(imgs[cnt], (1, 2, 0)))\n            axs[i, j].axis('off')\n            cnt += 1\n    \n    fig.savefig(f\"gan_samples/{act_name}/epoch_{epoch}.png\")\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:29.415200Z","iopub.execute_input":"2025-04-28T17:01:29.415442Z","iopub.status.idle":"2025-04-28T17:01:29.421580Z","shell.execute_reply.started":"2025-04-28T17:01:29.415425Z","shell.execute_reply":"2025-04-28T17:01:29.420787Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### 6. VISUALIZATION","metadata":{}},{"cell_type":"code","source":"def plot_benchmark_results(results, model_name, dataset_name):\n    \"\"\"Plot the results of a benchmark\"\"\"\n    metrics = ['train_loss', 'train_acc', 'test_acc', 'grad_norm', 'time']\n    \n    plt.figure(figsize=(20, 15))\n    \n    for i, metric in enumerate(metrics, 1):\n        plt.subplot(3, 2, i)\n        for name, res in results.items():\n            plt.plot(res[metric], label=name)\n        plt.title(f\"{model_name} - {dataset_name} - {metric.replace('_', ' ').title()}\")\n        plt.xlabel('Epoch')\n        plt.ylabel(metric.replace('_', ' ').title())\n        plt.grid(True, linestyle='--', alpha=0.7)\n        plt.legend()\n    \n    plt.tight_layout()\n    os.makedirs(\"results\", exist_ok=True)\n    plt.savefig(f\"results/{model_name}_{dataset_name}_comparison.png\")\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:32.897842Z","iopub.execute_input":"2025-04-28T17:01:32.898569Z","iopub.status.idle":"2025-04-28T17:01:32.904041Z","shell.execute_reply.started":"2025-04-28T17:01:32.898545Z","shell.execute_reply":"2025-04-28T17:01:32.903313Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def plot_gan_results(results, dataset_name):\n    \"\"\"Plot the results of a GAN benchmark\"\"\"\n    metrics = ['d_loss', 'g_loss', 'real_score', 'fake_score', 'time']\n    \n    plt.figure(figsize=(20, 15))\n    \n    for i, metric in enumerate(metrics, 1):\n        plt.subplot(3, 2, i)\n        for name, res in results.items():\n            plt.plot(res[metric], label=name)\n        plt.title(f\"GAN - {dataset_name} - {metric.replace('_', ' ').title()}\")\n        plt.xlabel('Epoch')\n        plt.ylabel(metric.replace('_', ' ').title())\n        plt.grid(True, linestyle='--', alpha=0.7)\n        plt.legend()\n    \n    plt.tight_layout()\n    os.makedirs(\"results\", exist_ok=True)\n    plt.savefig(f\"results/GAN_{dataset_name}_comparison.png\")\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:36.689712Z","iopub.execute_input":"2025-04-28T17:01:36.690266Z","iopub.status.idle":"2025-04-28T17:01:36.695475Z","shell.execute_reply.started":"2025-04-28T17:01:36.690239Z","shell.execute_reply":"2025-04-28T17:01:36.694752Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### 7. THEORETICAL ANALYSIS HELPER FUNCTIONS","metadata":{}},{"cell_type":"code","source":"def compute_lipschitz_estimate(activation, input_range=(-10, 10), num_samples=1000):\n    \"\"\"Empirically estimate the Lipschitz constant of an activation function\"\"\"\n    x = torch.linspace(input_range[0], input_range[1], num_samples).to(device)\n    y = activation(x)\n    \n    # Compute gradients using finite differences\n    grad = (y[1:] - y[:-1]) / (x[1:] - x[:-1])\n    \n    # Lipschitz constant is the maximum absolute gradient\n    return torch.max(torch.abs(grad)).item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:41.019758Z","iopub.execute_input":"2025-04-28T17:01:41.020346Z","iopub.status.idle":"2025-04-28T17:01:41.024550Z","shell.execute_reply.started":"2025-04-28T17:01:41.020323Z","shell.execute_reply":"2025-04-28T17:01:41.023988Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def check_differentiability(activation, input_range=(-10, 10), num_samples=1000):\n    \"\"\"Check if the activation function is differentiable at different points\"\"\"\n    x = torch.linspace(input_range[0], input_range[1], num_samples, requires_grad=True).to(device)\n    y = activation(x)\n    \n    try:\n        # Compute gradients\n        gradients = torch.autograd.grad(y.sum(), x, create_graph=True)[0]\n        \n        # Check if there are any NaN or Inf values\n        non_differentiable_points = torch.isnan(gradients) | torch.isinf(gradients)\n        \n        if torch.any(non_differentiable_points):\n            non_diff_inputs = x[non_differentiable_points]\n            return False, non_diff_inputs.detach().cpu().numpy()\n        else:\n            return True, []\n            \n    except Exception as e:\n        print(f\"Error in differentiability check: {e}\")\n        return False, []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:44.163552Z","iopub.execute_input":"2025-04-28T17:01:44.163824Z","iopub.status.idle":"2025-04-28T17:01:44.169519Z","shell.execute_reply.started":"2025-04-28T17:01:44.163805Z","shell.execute_reply":"2025-04-28T17:01:44.168757Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def analyze_gradient_flow(model, act_name, sample_data, device):\n    \"\"\"Analyze gradient flow through a model with a specific activation function\"\"\"\n    model.train()\n    \n    # Get a batch of data\n    inputs, targets = next(iter(sample_data))\n    inputs, targets = inputs.to(device), targets.to(device)\n    \n    # Forward pass\n    outputs = model(inputs)\n    loss = F.cross_entropy(outputs, targets)\n    \n    # Backward pass\n    loss.backward()\n    \n    # Collect gradients from different layers\n    grad_stats = {}\n    total_params = 0\n    vanishing_grad_params = 0\n    \n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.norm(2).item()\n            grad_stats[name] = grad_norm\n            \n            total_params += param.numel()\n            # Check for vanishing gradients (using a threshold)\n            vanishing_grad_params += torch.sum(torch.abs(param.grad) < 1e-8).item()\n    \n    vanishing_grad_ratio = vanishing_grad_params / total_params\n    \n    # Calculate gradient statistics\n    grad_norms = list(grad_stats.values())\n    mean_grad = np.mean(grad_norms)\n    std_grad = np.std(grad_norms)\n    max_grad = np.max(grad_norms)\n    min_grad = np.min(grad_norms)\n    \n    return {\n        'activation': act_name,\n        'mean_grad': mean_grad,\n        'std_grad': std_grad,\n        'max_grad': max_grad,\n        'min_grad': min_grad,\n        'vanishing_ratio': vanishing_grad_ratio,\n        'detailed_grads': grad_stats\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:48.300310Z","iopub.execute_input":"2025-04-28T17:01:48.300877Z","iopub.status.idle":"2025-04-28T17:01:48.306873Z","shell.execute_reply.started":"2025-04-28T17:01:48.300854Z","shell.execute_reply":"2025-04-28T17:01:48.306160Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def analyze_all_activations():\n    \"\"\"Perform theoretical analysis on all activation functions\"\"\"\n    print(\"\\n===== Theoretical Analysis of Activation Functions =====\")\n    \n    results = {}\n    input_range = (-5, 5)\n    \n    for name, act_fn in activations.items():\n            \n        print(f\"\\nAnalyzing {name}...\")\n        activation = act_fn().to(device)\n        \n        # 1. Lipschitz continuity\n        lipschitz = compute_lipschitz_estimate(activation, input_range)\n        \n        # 2. Differentiability\n        diff_status, non_diff_points = check_differentiability(activation, input_range)\n        \n        # 3. Plot activation function and its derivative\n        x = torch.linspace(input_range[0], input_range[1], 1000, requires_grad=True).to(device)\n        with torch.no_grad():\n            y = activation(x)\n        \n        # Try to compute the derivative\n        x_grad = torch.linspace(input_range[0], input_range[1], 1000, requires_grad=True).to(device)\n        y_grad = activation(x_grad)\n        try:\n            dy_dx = torch.autograd.grad(y_grad.sum(), x_grad)[0]\n            has_derivative = True\n        except:\n            has_derivative = False\n            dy_dx = torch.zeros_like(x_grad)\n        \n        # Store results\n        results[name] = {\n            'lipschitz': lipschitz,\n            'differentiable': diff_status,\n            'non_diff_points': non_diff_points if not diff_status else [],\n            'x': x.detach().cpu().numpy(),\n            'y': y.detach().cpu().numpy(),\n            'dy_dx': dy_dx.detach().cpu().numpy() if has_derivative else None\n        }\n        \n        print(f\"  - Lipschitz constant estimate: {lipschitz:.4f}\")\n        print(f\"  - Differentiable: {diff_status}\")\n        if not diff_status and len(non_diff_points) > 0:\n            print(f\"  - Non-differentiable points: {non_diff_points}\")\n    \n    # Create activation function comparison plots\n    plot_activation_comparisons(results)\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:54.132458Z","iopub.execute_input":"2025-04-28T17:01:54.133030Z","iopub.status.idle":"2025-04-28T17:01:54.140371Z","shell.execute_reply.started":"2025-04-28T17:01:54.132978Z","shell.execute_reply":"2025-04-28T17:01:54.138974Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def plot_activation_comparisons(results):\n    \"\"\"Plot and compare all activation functions and their derivatives\"\"\"\n    plt.figure(figsize=(20, 10))\n    \n    # Plot activation functions\n    plt.subplot(1, 2, 1)\n    for name, data in results.items():\n        plt.plot(data['x'], data['y'], label=name)\n    plt.title('Activation Functions')\n    plt.xlabel('Input')\n    plt.ylabel('Output')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.legend()\n    \n    # Plot derivatives\n    plt.subplot(1, 2, 2)\n    for name, data in results.items():\n        if data['dy_dx'] is not None:\n            plt.plot(data['x'], data['dy_dx'], label=f\"{name} derivative\")\n    plt.title('Derivatives of Activation Functions')\n    plt.xlabel('Input')\n    plt.ylabel('Derivative')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.legend()\n    \n    plt.tight_layout()\n    os.makedirs(\"results\", exist_ok=True)\n    plt.savefig(\"results/activation_function_comparison.png\")\n    plt.close()\n    \n    # Create individual plots for each activation\n    for name, data in results.items():\n        plt.figure(figsize=(12, 6))\n        \n        plt.subplot(1, 2, 1)\n        plt.plot(data['x'], data['y'])\n        plt.title(f'{name} Activation Function')\n        plt.xlabel('Input')\n        plt.ylabel('Output')\n        plt.grid(True, linestyle='--', alpha=0.7)\n        \n        plt.subplot(1, 2, 2)\n        if data['dy_dx'] is not None:\n            plt.plot(data['x'], data['dy_dx'])\n            plt.title(f'{name} Derivative')\n            plt.xlabel('Input')\n            plt.ylabel('Derivative')\n            plt.grid(True, linestyle='--', alpha=0.7)\n        else:\n            plt.text(0.5, 0.5, \"Derivative not available\", \n                     horizontalalignment='center', verticalalignment='center')\n            plt.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig(f\"results/{name}_function_analysis.png\")\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:01:58.957599Z","iopub.execute_input":"2025-04-28T17:01:58.958122Z","iopub.status.idle":"2025-04-28T17:01:58.966495Z","shell.execute_reply.started":"2025-04-28T17:01:58.958097Z","shell.execute_reply":"2025-04-28T17:01:58.965703Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def run_gradient_flow_analysis(datasets, model_types, act_names):\n    \"\"\"Analyze gradient flow for different models, datasets, and activations\"\"\"\n    print(\"\\n===== Gradient Flow Analysis =====\")\n    \n    results = {}\n    \n    for dataset_name in datasets:\n        train_loader, _, in_channels, num_classes, img_size = get_dataset_loaders(dataset_name)\n        \n        for model_name in model_types:\n            model_results = {}\n            \n            for act_name in act_names:\n                print(f\"\\nAnalyzing gradient flow for {model_name} with {act_name} on {dataset_name}...\")\n                \n                # Initialize model\n                if model_name == 'SimpleCNN':\n                    model = SimpleCNN(act_name, in_channels=in_channels, num_classes=num_classes).to(device)\n                elif model_name == 'ResNet':\n                    model = ResNet(act_name, num_blocks=[2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels).to(device)\n                elif model_name == 'Transformer':\n                    input_dim = in_channels * img_size * img_size\n                    model = SimpleTransformer(act_name, input_dim=input_dim, num_classes=num_classes).to(device)\n                else:\n                    continue\n                \n                # Analyze gradient flow\n                grad_flow = analyze_gradient_flow(model, act_name, train_loader, device)\n                model_results[act_name] = grad_flow\n                \n                print(f\"  - Mean gradient: {grad_flow['mean_grad']:.6f}\")\n                print(f\"  - Gradient std: {grad_flow['std_grad']:.6f}\")\n                print(f\"  - Vanishing gradient ratio: {grad_flow['vanishing_ratio']:.2%}\")\n            \n            results.setdefault(dataset_name, {})[model_name] = model_results\n    \n    # Plot gradient flow results\n    plot_gradient_flow_results(results)\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:02:04.837889Z","iopub.execute_input":"2025-04-28T17:02:04.838423Z","iopub.status.idle":"2025-04-28T17:02:04.844689Z","shell.execute_reply.started":"2025-04-28T17:02:04.838398Z","shell.execute_reply":"2025-04-28T17:02:04.844063Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def plot_gradient_flow_results(results):\n    \"\"\"Plot gradient flow analysis results\"\"\"\n    metrics = ['mean_grad', 'std_grad', 'max_grad', 'min_grad', 'vanishing_ratio']\n    metric_titles = {\n        'mean_grad': 'Mean Gradient Magnitude',\n        'std_grad': 'Gradient Standard Deviation',\n        'max_grad': 'Maximum Gradient Magnitude',\n        'min_grad': 'Minimum Gradient Magnitude',\n        'vanishing_ratio': 'Vanishing Gradient Ratio'\n    }\n    \n    for dataset_name, dataset_results in results.items():\n        for model_name, model_results in dataset_results.items():\n            plt.figure(figsize=(20, 15))\n            \n            for i, metric in enumerate(metrics, 1):\n                plt.subplot(3, 2, i)\n                \n                act_names = list(model_results.keys())\n                values = [model_results[act]['vanishing_ratio']*100 if metric == 'vanishing_ratio' \n                          else model_results[act][metric] for act in act_names]\n                \n                plt.bar(act_names, values)\n                plt.title(f\"{metric_titles[metric]} - {model_name} on {dataset_name}\")\n                plt.ylabel(metric_titles[metric])\n                plt.xticks(rotation=45)\n                plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n                \n                # Add value labels\n                for j, v in enumerate(values):\n                    plt.text(j, v + max(values)*0.01, f\"{v:.4f}\" if metric != 'vanishing_ratio' else f\"{v:.1f}%\", \n                             ha='center', va='bottom')\n            \n            plt.tight_layout()\n            os.makedirs(\"results/gradient_flow\", exist_ok=True)\n            plt.savefig(f\"results/gradient_flow/{dataset_name}_{model_name}_gradient_analysis.png\")\n            plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:02:10.200196Z","iopub.execute_input":"2025-04-28T17:02:10.200758Z","iopub.status.idle":"2025-04-28T17:02:10.207535Z","shell.execute_reply.started":"2025-04-28T17:02:10.200735Z","shell.execute_reply":"2025-04-28T17:02:10.206838Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### 8. MAIN EXECUTION PIPELINE","metadata":{}},{"cell_type":"code","source":"def main():\n    # Create directories for results\n    os.makedirs(\"results\", exist_ok=True)\n    os.makedirs(\"gan_samples\", exist_ok=True)\n    \n    # Activation functions\n    act_names = ['ReLU', 'LeakyReLU', 'PReLU', 'SoftReLU', 'ESReLU', 'APU']\n    \n    # Perform theoretical analysis\n    activation_analysis = analyze_all_activations()\n    \n    # Datasets to test\n    datasets = ['MNIST', 'CIFAR10', 'EMNIST']\n    \n    # Models to test\n    models = ['SimpleCNN', 'ResNet', 'Transformer']\n    \n    # Initialize counters for total models\n    total_initialized = 0\n    total_used = 0\n    \n    # Run benchmarks for each model and dataset combination\n    results = {}\n    \n    for dataset in datasets:\n        results[dataset] = {}\n        \n        for model in models:\n            print(f\"\\n\\n{'='*50}\")\n            print(f\"Running benchmark for {model} on {dataset}\")\n            print(f\"{'='*50}\")\n            \n            # Run benchmark with fewer epochs for quick results\n            benchmark_results = run_activation_benchmark(model, dataset, act_names, epochs=5)\n            results[dataset][model] = benchmark_results\n            \n            # Assume run_activation_benchmark initializes and uses len(act_names) models per call\n            initialized_count = len(act_names)  # One model per activation\n            used_count = len(act_names)  # All initialized models are used, assuming no errors\n            total_initialized += initialized_count\n            total_used += used_count\n            print(f\"Models initialized for {model} on {dataset}: {initialized_count}\")\n            print(f\"Models used for {model} on {dataset}: {used_count}\")\n    \n    # Run GAN benchmarks\n    gan_results = {}\n    for dataset in ['MNIST', 'EMNIST']:  # GANs are typically simpler to train on MNIST-like datasets\n        print(f\"\\n\\n{'='*50}\")\n        print(f\"Running GAN benchmark on {dataset}\")\n        print(f\"{'='*50}\")\n        \n        # Run GAN benchmark with fewer epochs for quick results\n        gan_benchmark = run_gan_benchmark(dataset, act_names, epochs=5)\n        gan_results[dataset] = gan_benchmark\n        \n        # Assume run_gan_benchmark initializes and uses 2 models (Generator, Discriminator) per activation\n        initialized_count = 2 * len(act_names)  # Two models per activation\n        used_count = 2 * len(act_names)  # All initialized models are used\n        total_initialized += initialized_count\n        total_used += used_count\n        print(f\"Models initialized for GAN on {dataset}: {initialized_count}\")\n        print(f\"Models used for GAN on {dataset}: {used_count}\")\n    \n    # Run gradient flow analysis\n    gradient_flow_results = run_gradient_flow_analysis(datasets, models, act_names)\n    \n    # Print total counts\n    print(\"\\n\\n===== All benchmarks completed =====\")\n    print(f\"Total models initialized: {total_initialized}\")\n    print(f\"Total models used: {total_used}\")\n    print(\"Results saved to 'results/' directory\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:02:21.820859Z","iopub.execute_input":"2025-04-28T17:02:21.821456Z","iopub.status.idle":"2025-04-28T21:03:03.942644Z","shell.execute_reply.started":"2025-04-28T17:02:21.821432Z","shell.execute_reply":"2025-04-28T21:03:03.942028Z"}},"outputs":[{"name":"stdout","text":"\n===== Theoretical Analysis of Activation Functions =====\n\nAnalyzing ReLU...\n  - Lipschitz constant estimate: 1.0000\n  - Differentiable: True\n\nAnalyzing LeakyReLU...\n  - Lipschitz constant estimate: 1.0000\n  - Differentiable: True\n\nAnalyzing PReLU...\n  - Lipschitz constant estimate: 1.0000\n  - Differentiable: True\n\nAnalyzing SoftReLU...\n  - Lipschitz constant estimate: 1.0998\n  - Differentiable: True\n\nAnalyzing ESReLU...\n  - Lipschitz constant estimate: 1.0000\n  - Differentiable: True\n\nAnalyzing APU...\n  - Lipschitz constant estimate: 1.0153\n  - Differentiable: True\n\n\n==================================================\nRunning benchmark for SimpleCNN on MNIST\n==================================================\n\n===== Running SimpleCNN on MNIST =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9.91M/9.91M [00:00<00:00, 38.2MB/s]\n100%|██████████| 28.9k/28.9k [00:00<00:00, 1.14MB/s]\n100%|██████████| 1.65M/1.65M [00:00<00:00, 10.4MB/s]\n100%|██████████| 4.54k/4.54k [00:00<00:00, 7.99MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\n### Activation: ReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1133, train_acc=96.5%, test_acc=98.5%, grad_norm=0.29, time=17.90s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0375, train_acc=98.8%, test_acc=98.8%, grad_norm=0.16, time=17.13s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0230, train_acc=99.3%, test_acc=98.9%, grad_norm=0.13, time=17.22s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0154, train_acc=99.5%, test_acc=98.9%, grad_norm=0.10, time=17.06s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0114, train_acc=99.6%, test_acc=98.6%, grad_norm=0.09, time=17.10s\n\n### Activation: LeakyReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1179, train_acc=96.5%, test_acc=98.9%, grad_norm=0.27, time=16.88s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0361, train_acc=98.9%, test_acc=99.0%, grad_norm=0.15, time=17.01s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0224, train_acc=99.3%, test_acc=98.5%, grad_norm=0.12, time=17.26s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0152, train_acc=99.5%, test_acc=98.8%, grad_norm=0.10, time=17.05s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0098, train_acc=99.7%, test_acc=98.9%, grad_norm=0.07, time=16.80s\n\n### Activation: PReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1100, train_acc=96.7%, test_acc=98.7%, grad_norm=0.20, time=18.03s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0331, train_acc=99.0%, test_acc=98.7%, grad_norm=0.13, time=18.16s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0201, train_acc=99.3%, test_acc=98.5%, grad_norm=0.10, time=18.06s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0138, train_acc=99.6%, test_acc=98.8%, grad_norm=0.08, time=18.07s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0103, train_acc=99.7%, test_acc=98.9%, grad_norm=0.07, time=18.04s\n\n### Activation: SoftReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1055, train_acc=96.7%, test_acc=98.6%, grad_norm=0.23, time=17.80s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0328, train_acc=99.0%, test_acc=98.7%, grad_norm=0.14, time=17.86s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0190, train_acc=99.4%, test_acc=98.7%, grad_norm=0.11, time=17.88s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0139, train_acc=99.6%, test_acc=98.8%, grad_norm=0.09, time=17.94s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0125, train_acc=99.6%, test_acc=98.9%, grad_norm=0.09, time=17.62s\n\n### Activation: ESReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1112, train_acc=96.6%, test_acc=98.6%, grad_norm=0.24, time=19.86s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0343, train_acc=98.9%, test_acc=98.8%, grad_norm=0.14, time=19.72s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0209, train_acc=99.3%, test_acc=98.5%, grad_norm=0.11, time=19.85s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0131, train_acc=99.6%, test_acc=98.3%, grad_norm=0.09, time=19.93s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0113, train_acc=99.6%, test_acc=98.7%, grad_norm=0.08, time=20.03s\n\n### Activation: APU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1675, train_acc=94.9%, test_acc=97.5%, grad_norm=0.20, time=21.14s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0655, train_acc=98.0%, test_acc=97.9%, grad_norm=0.15, time=21.04s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0409, train_acc=98.6%, test_acc=98.5%, grad_norm=0.16, time=20.79s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0298, train_acc=99.0%, test_acc=98.4%, grad_norm=0.18, time=21.00s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0234, train_acc=99.2%, test_acc=98.4%, grad_norm=0.21, time=20.92s\nModels initialized for SimpleCNN on MNIST: 6\nModels used for SimpleCNN on MNIST: 6\n\n\n==================================================\nRunning benchmark for ResNet on MNIST\n==================================================\n\n===== Running ResNet on MNIST =====\n\n### Activation: ReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1047, train_acc=96.8%, test_acc=98.8%, grad_norm=0.06, time=58.62s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0445, train_acc=98.7%, test_acc=98.5%, grad_norm=0.03, time=58.36s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0341, train_acc=99.0%, test_acc=98.6%, grad_norm=0.02, time=58.27s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0281, train_acc=99.2%, test_acc=98.9%, grad_norm=0.01, time=58.38s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0261, train_acc=99.2%, test_acc=98.9%, grad_norm=0.01, time=58.28s\n\n### Activation: LeakyReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1101, train_acc=96.6%, test_acc=98.4%, grad_norm=0.06, time=58.46s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0437, train_acc=98.7%, test_acc=98.6%, grad_norm=0.02, time=58.37s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0371, train_acc=98.9%, test_acc=99.0%, grad_norm=0.02, time=58.43s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0288, train_acc=99.1%, test_acc=99.0%, grad_norm=0.01, time=58.38s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0241, train_acc=99.2%, test_acc=99.0%, grad_norm=0.01, time=58.30s\n\n### Activation: PReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1071, train_acc=96.7%, test_acc=95.1%, grad_norm=0.05, time=65.86s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0443, train_acc=98.7%, test_acc=98.8%, grad_norm=0.02, time=65.80s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0340, train_acc=99.0%, test_acc=99.3%, grad_norm=0.02, time=65.89s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0248, train_acc=99.2%, test_acc=99.2%, grad_norm=0.01, time=65.89s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0241, train_acc=99.3%, test_acc=99.3%, grad_norm=0.01, time=65.85s\n\n### Activation: SoftReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1079, train_acc=96.7%, test_acc=97.2%, grad_norm=0.05, time=65.62s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0427, train_acc=98.7%, test_acc=99.2%, grad_norm=0.02, time=65.68s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0333, train_acc=99.0%, test_acc=99.2%, grad_norm=0.02, time=65.61s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0273, train_acc=99.1%, test_acc=99.3%, grad_norm=0.01, time=65.66s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0223, train_acc=99.3%, test_acc=98.9%, grad_norm=0.01, time=65.70s\n\n### Activation: ESReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.1097, train_acc=96.5%, test_acc=98.4%, grad_norm=0.06, time=78.07s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0446, train_acc=98.7%, test_acc=98.6%, grad_norm=0.02, time=77.93s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0356, train_acc=98.9%, test_acc=98.8%, grad_norm=0.02, time=78.02s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0289, train_acc=99.1%, test_acc=99.0%, grad_norm=0.01, time=78.09s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0250, train_acc=99.2%, test_acc=99.0%, grad_norm=0.01, time=77.85s\n\n### Activation: APU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.3276, train_acc=90.2%, test_acc=97.5%, grad_norm=0.05, time=96.60s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.0725, train_acc=97.7%, test_acc=98.4%, grad_norm=0.03, time=96.58s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0488, train_acc=98.5%, test_acc=98.5%, grad_norm=0.02, time=96.54s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0402, train_acc=98.7%, test_acc=98.7%, grad_norm=0.02, time=96.57s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0347, train_acc=99.0%, test_acc=98.9%, grad_norm=0.02, time=96.57s\nModels initialized for ResNet on MNIST: 6\nModels used for ResNet on MNIST: 6\n\n\n==================================================\nRunning benchmark for Transformer on MNIST\n==================================================\n\n===== Running Transformer on MNIST =====\n\n### Activation: ReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.2441, train_acc=92.5%, test_acc=95.9%, grad_norm=0.14, time=17.41s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.1153, train_acc=96.3%, test_acc=96.9%, grad_norm=0.11, time=17.33s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0911, train_acc=97.2%, test_acc=97.0%, grad_norm=0.10, time=17.44s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0765, train_acc=97.6%, test_acc=97.5%, grad_norm=0.09, time=17.36s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0727, train_acc=97.7%, test_acc=97.0%, grad_norm=0.10, time=17.35s\n\n### Activation: LeakyReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.2407, train_acc=92.7%, test_acc=96.6%, grad_norm=0.13, time=17.47s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.1172, train_acc=96.2%, test_acc=95.7%, grad_norm=0.11, time=17.25s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0987, train_acc=96.9%, test_acc=96.7%, grad_norm=0.11, time=17.46s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0798, train_acc=97.5%, test_acc=97.1%, grad_norm=0.10, time=17.24s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0673, train_acc=97.8%, test_acc=97.4%, grad_norm=0.09, time=17.30s\n\n### Activation: PReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.2421, train_acc=92.6%, test_acc=94.0%, grad_norm=0.14, time=17.78s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.1189, train_acc=96.4%, test_acc=96.6%, grad_norm=0.11, time=17.83s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0963, train_acc=97.0%, test_acc=97.0%, grad_norm=0.11, time=17.83s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0843, train_acc=97.3%, test_acc=97.0%, grad_norm=0.11, time=17.93s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0698, train_acc=97.8%, test_acc=97.8%, grad_norm=0.09, time=17.91s\n\n### Activation: SoftReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.2607, train_acc=92.0%, test_acc=95.2%, grad_norm=0.13, time=17.52s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.1309, train_acc=96.0%, test_acc=96.4%, grad_norm=0.11, time=17.75s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0959, train_acc=97.0%, test_acc=96.8%, grad_norm=0.10, time=17.58s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0833, train_acc=97.4%, test_acc=96.6%, grad_norm=0.10, time=17.64s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0674, train_acc=97.8%, test_acc=97.5%, grad_norm=0.08, time=17.55s\n\n### Activation: ESReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.2488, train_acc=92.4%, test_acc=95.2%, grad_norm=0.13, time=18.46s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.1237, train_acc=96.1%, test_acc=96.3%, grad_norm=0.11, time=18.48s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.0972, train_acc=97.0%, test_acc=96.3%, grad_norm=0.10, time=18.44s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.0838, train_acc=97.4%, test_acc=97.5%, grad_norm=0.10, time=18.44s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.0853, train_acc=97.2%, test_acc=97.7%, grad_norm=0.11, time=17.96s\n\n### Activation: APU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.3787, train_acc=88.6%, test_acc=92.3%, grad_norm=0.13, time=18.94s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.2115, train_acc=93.6%, test_acc=95.5%, grad_norm=0.11, time=18.82s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.1477, train_acc=95.6%, test_acc=95.2%, grad_norm=0.10, time=18.83s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.1189, train_acc=96.4%, test_acc=96.5%, grad_norm=0.10, time=18.92s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.1050, train_acc=96.7%, test_acc=96.3%, grad_norm=0.10, time=18.89s\nModels initialized for Transformer on MNIST: 6\nModels used for Transformer on MNIST: 6\n\n\n==================================================\nRunning benchmark for SimpleCNN on CIFAR10\n==================================================\n\n===== Running SimpleCNN on CIFAR10 =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:02<00:00, 74.5MB/s] \n","output_type":"stream"},{"name":"stdout","text":"\n### Activation: ReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.4909, train_acc=45.8%, test_acc=60.5%, grad_norm=0.90, time=23.58s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.1584, train_acc=58.9%, test_acc=65.8%, grad_norm=1.00, time=23.49s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=1.0586, train_acc=62.6%, test_acc=68.2%, grad_norm=1.00, time=23.51s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.9945, train_acc=64.9%, test_acc=69.2%, grad_norm=1.01, time=23.69s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.9483, train_acc=66.5%, test_acc=69.1%, grad_norm=1.01, time=23.28s\n\n### Activation: LeakyReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.4321, train_acc=47.7%, test_acc=62.0%, grad_norm=0.92, time=23.39s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.1043, train_acc=60.5%, test_acc=67.3%, grad_norm=1.01, time=23.45s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.9916, train_acc=65.1%, test_acc=69.7%, grad_norm=1.01, time=23.36s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.9198, train_acc=67.6%, test_acc=71.4%, grad_norm=1.00, time=23.19s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.8626, train_acc=69.8%, test_acc=71.5%, grad_norm=0.99, time=23.33s\n\n### Activation: PReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.3962, train_acc=49.4%, test_acc=63.6%, grad_norm=0.79, time=24.60s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.0784, train_acc=62.0%, test_acc=67.5%, grad_norm=0.89, time=24.48s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.9551, train_acc=66.1%, test_acc=68.3%, grad_norm=0.93, time=24.51s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.8848, train_acc=68.8%, test_acc=73.1%, grad_norm=0.96, time=24.61s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.8298, train_acc=70.7%, test_acc=73.2%, grad_norm=0.98, time=24.78s\n\n### Activation: SoftReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.4353, train_acc=48.3%, test_acc=62.4%, grad_norm=0.79, time=24.47s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.1179, train_acc=60.5%, test_acc=66.6%, grad_norm=0.93, time=24.58s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=1.0168, train_acc=64.4%, test_acc=68.9%, grad_norm=0.95, time=24.43s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.9490, train_acc=66.5%, test_acc=70.3%, grad_norm=0.95, time=24.60s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.9068, train_acc=68.3%, test_acc=71.8%, grad_norm=0.98, time=24.79s\n\n### Activation: ESReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.4110, train_acc=48.9%, test_acc=62.6%, grad_norm=0.90, time=26.97s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.0860, train_acc=61.5%, test_acc=66.5%, grad_norm=0.99, time=27.10s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.9690, train_acc=65.6%, test_acc=69.6%, grad_norm=1.01, time=27.00s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.8990, train_acc=68.5%, test_acc=71.7%, grad_norm=1.00, time=26.91s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.8498, train_acc=70.2%, test_acc=73.7%, grad_norm=1.01, time=26.84s\n\n### Activation: APU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.5606, train_acc=44.8%, test_acc=57.1%, grad_norm=0.55, time=30.10s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.2629, train_acc=55.2%, test_acc=62.4%, grad_norm=0.70, time=29.95s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=1.1636, train_acc=58.8%, test_acc=64.6%, grad_norm=0.89, time=30.05s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=1.1226, train_acc=60.7%, test_acc=65.8%, grad_norm=1.05, time=29.81s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=1.0693, train_acc=62.4%, test_acc=66.3%, grad_norm=1.06, time=29.76s\nModels initialized for SimpleCNN on CIFAR10: 6\nModels used for SimpleCNN on CIFAR10: 6\n\n\n==================================================\nRunning benchmark for ResNet on CIFAR10\n==================================================\n\n===== Running ResNet on CIFAR10 =====\n\n### Activation: ReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.4600, train_acc=46.5%, test_acc=59.2%, grad_norm=0.15, time=60.43s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.9839, train_acc=65.0%, test_acc=69.7%, grad_norm=0.13, time=60.39s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.7746, train_acc=72.8%, test_acc=74.8%, grad_norm=0.12, time=60.45s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.6377, train_acc=77.9%, test_acc=78.8%, grad_norm=0.11, time=60.54s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.5477, train_acc=80.9%, test_acc=80.2%, grad_norm=0.10, time=60.39s\n\n### Activation: LeakyReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.4129, train_acc=48.2%, test_acc=62.0%, grad_norm=0.16, time=60.38s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.9437, train_acc=66.9%, test_acc=64.6%, grad_norm=0.13, time=60.79s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.7398, train_acc=74.1%, test_acc=74.5%, grad_norm=0.11, time=60.70s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.6171, train_acc=78.7%, test_acc=78.3%, grad_norm=0.11, time=60.62s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.5326, train_acc=81.6%, test_acc=81.2%, grad_norm=0.10, time=60.43s\n\n### Activation: PReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.4495, train_acc=46.7%, test_acc=58.6%, grad_norm=0.12, time=68.39s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.9351, train_acc=66.7%, test_acc=70.5%, grad_norm=0.11, time=68.31s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.6994, train_acc=75.6%, test_acc=76.4%, grad_norm=0.10, time=68.24s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.5560, train_acc=80.7%, test_acc=79.6%, grad_norm=0.10, time=68.27s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.4775, train_acc=83.6%, test_acc=81.6%, grad_norm=0.09, time=68.18s\n\n### Activation: SoftReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.5446, train_acc=42.5%, test_acc=57.3%, grad_norm=0.11, time=68.37s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.9868, train_acc=65.1%, test_acc=67.6%, grad_norm=0.10, time=68.39s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.7487, train_acc=73.6%, test_acc=75.6%, grad_norm=0.10, time=68.36s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.6067, train_acc=78.9%, test_acc=79.7%, grad_norm=0.09, time=68.30s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.5245, train_acc=81.9%, test_acc=81.8%, grad_norm=0.08, time=68.23s\n\n### Activation: ESReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.5199, train_acc=43.8%, test_acc=54.7%, grad_norm=0.15, time=82.73s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.0236, train_acc=63.3%, test_acc=66.9%, grad_norm=0.12, time=82.67s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.8204, train_acc=71.0%, test_acc=75.2%, grad_norm=0.11, time=82.59s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.6698, train_acc=76.7%, test_acc=76.5%, grad_norm=0.10, time=83.34s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.5804, train_acc=79.7%, test_acc=80.6%, grad_norm=0.10, time=83.07s\n\n### Activation: APU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.8697, train_acc=31.2%, test_acc=41.5%, grad_norm=0.07, time=102.62s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.3922, train_acc=49.3%, test_acc=54.8%, grad_norm=0.08, time=102.81s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=1.1221, train_acc=60.2%, test_acc=63.9%, grad_norm=0.12, time=102.65s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.9941, train_acc=64.9%, test_acc=68.2%, grad_norm=0.16, time=102.68s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.9112, train_acc=68.1%, test_acc=71.2%, grad_norm=0.20, time=102.95s\nModels initialized for ResNet on CIFAR10: 6\nModels used for ResNet on CIFAR10: 6\n\n\n==================================================\nRunning benchmark for Transformer on CIFAR10\n==================================================\n\n===== Running Transformer on CIFAR10 =====\n\n### Activation: ReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.9101, train_acc=30.3%, test_acc=36.0%, grad_norm=0.20, time=23.86s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.7805, train_acc=35.2%, test_acc=38.8%, grad_norm=0.21, time=23.43s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=1.7338, train_acc=37.1%, test_acc=39.3%, grad_norm=0.22, time=23.53s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=1.7077, train_acc=37.9%, test_acc=39.5%, grad_norm=0.24, time=23.48s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=1.6639, train_acc=39.8%, test_acc=40.6%, grad_norm=0.23, time=23.84s\n\n### Activation: LeakyReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.9189, train_acc=30.0%, test_acc=35.4%, grad_norm=0.19, time=23.74s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.7806, train_acc=35.1%, test_acc=37.3%, grad_norm=0.20, time=23.50s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=1.7233, train_acc=37.4%, test_acc=40.3%, grad_norm=0.21, time=23.39s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=1.6780, train_acc=39.3%, test_acc=38.5%, grad_norm=0.20, time=23.40s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=1.6672, train_acc=39.5%, test_acc=41.9%, grad_norm=0.22, time=23.54s\n\n### Activation: PReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.9129, train_acc=29.9%, test_acc=36.2%, grad_norm=0.19, time=23.77s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.7658, train_acc=36.1%, test_acc=39.3%, grad_norm=0.20, time=23.81s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=1.7130, train_acc=37.8%, test_acc=36.9%, grad_norm=0.21, time=23.74s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=1.6827, train_acc=39.0%, test_acc=42.1%, grad_norm=0.22, time=23.67s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=1.6440, train_acc=40.5%, test_acc=42.0%, grad_norm=0.21, time=23.66s\n\n### Activation: SoftReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.9241, train_acc=29.8%, test_acc=35.7%, grad_norm=0.18, time=23.51s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.7861, train_acc=35.1%, test_acc=38.5%, grad_norm=0.20, time=23.43s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=1.7252, train_acc=37.5%, test_acc=40.9%, grad_norm=0.20, time=23.41s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=1.6865, train_acc=39.0%, test_acc=41.7%, grad_norm=0.20, time=23.83s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=1.6587, train_acc=39.9%, test_acc=40.9%, grad_norm=0.19, time=23.69s\n\n### Activation: ESReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.9289, train_acc=29.3%, test_acc=33.4%, grad_norm=0.20, time=24.04s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.8031, train_acc=34.6%, test_acc=35.7%, grad_norm=0.21, time=23.84s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=1.7526, train_acc=36.4%, test_acc=39.7%, grad_norm=0.22, time=23.95s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=1.6994, train_acc=38.0%, test_acc=41.5%, grad_norm=0.21, time=24.10s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=1.6695, train_acc=39.3%, test_acc=40.8%, grad_norm=0.20, time=24.01s\n\n### Activation: APU\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=2.0148, train_acc=25.9%, test_acc=29.2%, grad_norm=0.14, time=24.40s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=1.9047, train_acc=30.8%, test_acc=34.9%, grad_norm=0.14, time=24.67s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=1.8178, train_acc=34.2%, test_acc=38.9%, grad_norm=0.15, time=24.82s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=1.7415, train_acc=36.8%, test_acc=38.7%, grad_norm=0.15, time=24.65s\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=1.7314, train_acc=37.2%, test_acc=38.3%, grad_norm=0.17, time=24.68s\nModels initialized for Transformer on CIFAR10: 6\nModels used for Transformer on CIFAR10: 6\n\n\n==================================================\nRunning benchmark for SimpleCNN on EMNIST\n==================================================\n\n===== Running SimpleCNN on EMNIST =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 562M/562M [00:03<00:00, 148MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"\n### Activation: ReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.6310, train_acc=79.9%, test_acc=84.5%, grad_norm=0.75, time=31.97s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3770, train_acc=86.7%, test_acc=86.1%, grad_norm=0.56, time=31.15s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.3123, train_acc=88.5%, test_acc=86.2%, grad_norm=0.49, time=31.21s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2658, train_acc=89.8%, test_acc=86.8%, grad_norm=0.46, time=30.89s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.2269, train_acc=91.1%, test_acc=86.8%, grad_norm=0.43, time=30.96s\n\n### Activation: LeakyReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.5945, train_acc=80.8%, test_acc=85.5%, grad_norm=0.67, time=30.73s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3513, train_acc=87.4%, test_acc=86.2%, grad_norm=0.50, time=30.83s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.2867, train_acc=89.2%, test_acc=87.1%, grad_norm=0.44, time=30.89s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2374, train_acc=90.7%, test_acc=86.6%, grad_norm=0.40, time=31.15s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.1969, train_acc=92.0%, test_acc=86.8%, grad_norm=0.38, time=31.38s\n\n### Activation: PReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.5852, train_acc=81.1%, test_acc=85.6%, grad_norm=0.53, time=33.36s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3517, train_acc=87.4%, test_acc=86.1%, grad_norm=0.47, time=33.05s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.2884, train_acc=89.2%, test_acc=87.0%, grad_norm=0.44, time=33.24s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2371, train_acc=90.7%, test_acc=87.1%, grad_norm=0.42, time=33.05s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.1949, train_acc=92.1%, test_acc=86.8%, grad_norm=0.40, time=33.22s\n\n### Activation: SoftReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.5772, train_acc=81.3%, test_acc=85.0%, grad_norm=0.60, time=32.79s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3531, train_acc=87.4%, test_acc=85.8%, grad_norm=0.49, time=32.84s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.2872, train_acc=89.2%, test_acc=86.5%, grad_norm=0.45, time=32.90s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2381, train_acc=90.7%, test_acc=86.7%, grad_norm=0.43, time=32.84s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.1964, train_acc=92.0%, test_acc=86.3%, grad_norm=0.41, time=32.98s\n\n### Activation: ESReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.5893, train_acc=81.0%, test_acc=85.7%, grad_norm=0.64, time=36.39s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3532, train_acc=87.3%, test_acc=86.2%, grad_norm=0.48, time=36.79s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.2884, train_acc=89.1%, test_acc=86.8%, grad_norm=0.44, time=36.60s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2391, train_acc=90.7%, test_acc=86.4%, grad_norm=0.41, time=36.64s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.1992, train_acc=92.0%, test_acc=86.5%, grad_norm=0.39, time=36.78s\n\n### Activation: APU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.7308, train_acc=77.3%, test_acc=82.4%, grad_norm=0.42, time=38.60s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.4845, train_acc=83.6%, test_acc=83.1%, grad_norm=0.53, time=38.70s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.4405, train_acc=84.8%, test_acc=83.6%, grad_norm=0.93, time=38.51s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.4349, train_acc=84.9%, test_acc=80.3%, grad_norm=1.72, time=38.60s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.4302, train_acc=85.0%, test_acc=83.3%, grad_norm=2.16, time=38.45s\nModels initialized for SimpleCNN on EMNIST: 6\nModels used for SimpleCNN on EMNIST: 6\n\n\n==================================================\nRunning benchmark for ResNet on EMNIST\n==================================================\n\n===== Running ResNet on EMNIST =====\n\n### Activation: ReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.5110, train_acc=82.5%, test_acc=86.2%, grad_norm=0.09, time=109.23s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3411, train_acc=87.4%, test_acc=87.2%, grad_norm=0.05, time=109.27s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.3032, train_acc=88.7%, test_acc=88.3%, grad_norm=0.04, time=109.33s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2792, train_acc=89.4%, test_acc=88.3%, grad_norm=0.03, time=109.16s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.2571, train_acc=90.1%, test_acc=89.3%, grad_norm=0.03, time=109.29s\n\n### Activation: LeakyReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.5122, train_acc=82.5%, test_acc=86.6%, grad_norm=0.08, time=109.65s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3421, train_acc=87.5%, test_acc=86.8%, grad_norm=0.05, time=109.52s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.3035, train_acc=88.6%, test_acc=87.9%, grad_norm=0.04, time=109.63s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2809, train_acc=89.3%, test_acc=89.0%, grad_norm=0.03, time=109.77s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.2585, train_acc=90.0%, test_acc=89.5%, grad_norm=0.03, time=109.58s\n\n### Activation: PReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.5061, train_acc=82.8%, test_acc=84.7%, grad_norm=0.07, time=123.43s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3323, train_acc=87.7%, test_acc=87.4%, grad_norm=0.04, time=123.45s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.2946, train_acc=88.9%, test_acc=88.6%, grad_norm=0.03, time=123.46s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2674, train_acc=89.8%, test_acc=88.5%, grad_norm=0.03, time=123.44s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.2448, train_acc=90.4%, test_acc=89.4%, grad_norm=0.03, time=123.55s\n\n### Activation: SoftReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.4955, train_acc=83.1%, test_acc=86.2%, grad_norm=0.07, time=122.87s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3295, train_acc=87.9%, test_acc=87.6%, grad_norm=0.04, time=122.96s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.2975, train_acc=88.8%, test_acc=88.7%, grad_norm=0.03, time=122.97s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2704, train_acc=89.7%, test_acc=88.2%, grad_norm=0.03, time=122.95s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.2477, train_acc=90.4%, test_acc=88.7%, grad_norm=0.03, time=122.82s\n\n### Activation: ESReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.5123, train_acc=82.4%, test_acc=84.2%, grad_norm=0.09, time=146.40s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3431, train_acc=87.4%, test_acc=86.8%, grad_norm=0.05, time=146.20s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.3070, train_acc=88.6%, test_acc=88.0%, grad_norm=0.04, time=146.33s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2804, train_acc=89.4%, test_acc=88.2%, grad_norm=0.03, time=146.37s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.2601, train_acc=89.9%, test_acc=89.0%, grad_norm=0.03, time=146.42s\n\n### Activation: APU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.7572, train_acc=76.2%, test_acc=85.0%, grad_norm=0.06, time=181.40s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.3717, train_acc=86.7%, test_acc=87.5%, grad_norm=0.04, time=181.58s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.3270, train_acc=87.9%, test_acc=87.5%, grad_norm=0.04, time=181.59s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.2983, train_acc=88.8%, test_acc=88.4%, grad_norm=0.04, time=181.61s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.2770, train_acc=89.4%, test_acc=88.1%, grad_norm=0.04, time=181.70s\nModels initialized for ResNet on EMNIST: 6\nModels used for ResNet on EMNIST: 6\n\n\n==================================================\nRunning benchmark for Transformer on EMNIST\n==================================================\n\n===== Running Transformer on EMNIST =====\n\n### Activation: ReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.8273, train_acc=74.5%, test_acc=80.5%, grad_norm=0.23, time=32.83s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.5311, train_acc=81.8%, test_acc=81.8%, grad_norm=0.22, time=32.75s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.4739, train_acc=83.5%, test_acc=82.8%, grad_norm=0.21, time=33.07s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.4394, train_acc=84.3%, test_acc=83.5%, grad_norm=0.21, time=32.79s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.4164, train_acc=85.1%, test_acc=82.9%, grad_norm=0.21, time=32.71s\n\n### Activation: LeakyReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.8271, train_acc=74.3%, test_acc=80.6%, grad_norm=0.23, time=33.30s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.5300, train_acc=81.8%, test_acc=80.9%, grad_norm=0.21, time=32.98s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.4634, train_acc=83.8%, test_acc=82.4%, grad_norm=0.20, time=32.67s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.4440, train_acc=84.2%, test_acc=83.6%, grad_norm=0.21, time=33.05s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.4332, train_acc=84.5%, test_acc=82.5%, grad_norm=0.23, time=32.90s\n\n### Activation: PReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.8208, train_acc=74.5%, test_acc=80.1%, grad_norm=0.23, time=33.51s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.5252, train_acc=82.0%, test_acc=81.7%, grad_norm=0.22, time=33.73s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.4722, train_acc=83.4%, test_acc=82.1%, grad_norm=0.22, time=33.65s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.4293, train_acc=84.7%, test_acc=84.1%, grad_norm=0.21, time=33.28s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.4054, train_acc=85.4%, test_acc=83.4%, grad_norm=0.21, time=33.89s\n\n### Activation: SoftReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.8469, train_acc=73.8%, test_acc=81.0%, grad_norm=0.21, time=33.36s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.5135, train_acc=82.3%, test_acc=82.7%, grad_norm=0.19, time=33.41s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.4470, train_acc=84.2%, test_acc=83.2%, grad_norm=0.18, time=33.66s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.4103, train_acc=85.2%, test_acc=84.3%, grad_norm=0.18, time=33.60s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.3830, train_acc=86.0%, test_acc=84.0%, grad_norm=0.18, time=33.51s\n\n### Activation: ESReLU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=0.8435, train_acc=73.9%, test_acc=80.0%, grad_norm=0.23, time=34.78s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.5341, train_acc=81.8%, test_acc=82.4%, grad_norm=0.21, time=34.69s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.4816, train_acc=83.2%, test_acc=83.0%, grad_norm=0.21, time=34.52s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.4441, train_acc=84.2%, test_acc=82.8%, grad_norm=0.21, time=34.09s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.4165, train_acc=85.0%, test_acc=83.7%, grad_norm=0.21, time=34.39s\n\n### Activation: APU\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: loss=1.0938, train_acc=67.6%, test_acc=77.1%, grad_norm=0.22, time=35.69s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: loss=0.6424, train_acc=78.8%, test_acc=78.5%, grad_norm=0.21, time=35.78s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: loss=0.5559, train_acc=81.3%, test_acc=81.0%, grad_norm=0.20, time=36.21s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: loss=0.4968, train_acc=82.8%, test_acc=81.9%, grad_norm=0.18, time=36.00s\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: loss=0.4790, train_acc=83.3%, test_acc=82.9%, grad_norm=0.19, time=36.22s\nModels initialized for Transformer on EMNIST: 6\nModels used for Transformer on EMNIST: 6\n\n\n==================================================\nRunning GAN benchmark on MNIST\n==================================================\n\n===== Running GAN on MNIST =====\n\n### Activation: ReLU\nEpoch 1/5: d_loss=0.1732, g_loss=3.8635, real_score=0.9083, fake_score=0.1765, time=15.00s\nEpoch 2/5: d_loss=0.0505, g_loss=6.0432, real_score=0.9728, fake_score=0.0394, time=15.28s\nEpoch 3/5: d_loss=0.0373, g_loss=6.6036, real_score=0.9808, fake_score=0.0259, time=14.97s\nEpoch 4/5: d_loss=0.0263, g_loss=7.0473, real_score=0.9866, fake_score=0.0172, time=15.35s\nEpoch 5/5: d_loss=0.0211, g_loss=7.6734, real_score=0.9898, fake_score=0.0130, time=15.07s\n\n### Activation: LeakyReLU\nEpoch 1/5: d_loss=0.1767, g_loss=3.7994, real_score=0.9040, fake_score=0.1791, time=15.18s\nEpoch 2/5: d_loss=0.0525, g_loss=5.8269, real_score=0.9709, fake_score=0.0413, time=15.23s\nEpoch 3/5: d_loss=0.0353, g_loss=6.4890, real_score=0.9818, fake_score=0.0241, time=15.32s\nEpoch 4/5: d_loss=0.0256, g_loss=6.9924, real_score=0.9870, fake_score=0.0169, time=15.06s\nEpoch 5/5: d_loss=0.0185, g_loss=7.6764, real_score=0.9908, fake_score=0.0117, time=15.18s\n\n### Activation: PReLU\nEpoch 1/5: d_loss=0.2208, g_loss=3.0233, real_score=0.8799, fake_score=0.2167, time=16.54s\nEpoch 2/5: d_loss=0.0580, g_loss=4.8653, real_score=0.9680, fake_score=0.0429, time=16.51s\nEpoch 3/5: d_loss=0.0341, g_loss=5.6322, real_score=0.9822, fake_score=0.0223, time=16.46s\nEpoch 4/5: d_loss=0.0267, g_loss=6.4048, real_score=0.9865, fake_score=0.0166, time=16.53s\nEpoch 5/5: d_loss=0.0225, g_loss=6.7471, real_score=0.9891, fake_score=0.0136, time=16.55s\n\n### Activation: SoftReLU\nEpoch 1/5: d_loss=0.1467, g_loss=3.1757, real_score=0.9452, fake_score=0.1694, time=15.94s\nEpoch 2/5: d_loss=0.0425, g_loss=5.2302, real_score=0.9768, fake_score=0.0285, time=15.98s\nEpoch 3/5: d_loss=0.0305, g_loss=5.8297, real_score=0.9839, fake_score=0.0172, time=15.85s\nEpoch 4/5: d_loss=0.0234, g_loss=6.1755, real_score=0.9885, fake_score=0.0118, time=16.00s\nEpoch 5/5: d_loss=0.0197, g_loss=6.6021, real_score=0.9905, fake_score=0.0101, time=15.73s\n\n### Activation: ESReLU\nEpoch 1/5: d_loss=0.1857, g_loss=3.5645, real_score=0.8987, fake_score=0.1854, time=18.42s\nEpoch 2/5: d_loss=0.0475, g_loss=5.4577, real_score=0.9732, fake_score=0.0362, time=18.50s\nEpoch 3/5: d_loss=0.0276, g_loss=6.1757, real_score=0.9849, fake_score=0.0185, time=18.54s\nEpoch 4/5: d_loss=0.0224, g_loss=7.0440, real_score=0.9886, fake_score=0.0136, time=18.48s\nEpoch 5/5: d_loss=0.0176, g_loss=7.3285, real_score=0.9913, fake_score=0.0100, time=18.41s\n\n### Activation: APU\nEpoch 1/5: d_loss=0.4207, g_loss=1.7043, real_score=0.7694, fake_score=0.3020, time=20.84s\nEpoch 2/5: d_loss=0.2392, g_loss=2.6166, real_score=0.8611, fake_score=0.1524, time=21.02s\nEpoch 3/5: d_loss=0.1105, g_loss=3.6836, real_score=0.9436, fake_score=0.0616, time=21.14s\nEpoch 4/5: d_loss=0.0606, g_loss=4.4953, real_score=0.9714, fake_score=0.0314, time=20.85s\nEpoch 5/5: d_loss=0.0337, g_loss=5.3919, real_score=0.9848, fake_score=0.0171, time=21.00s\nModels initialized for GAN on MNIST: 12\nModels used for GAN on MNIST: 12\n\n\n==================================================\nRunning GAN benchmark on EMNIST\n==================================================\n\n===== Running GAN on EMNIST =====\n\n### Activation: ReLU\nEpoch 1/5: d_loss=0.1126, g_loss=4.8181, real_score=0.9450, fake_score=0.1096, time=28.45s\nEpoch 2/5: d_loss=0.0361, g_loss=6.4510, real_score=0.9823, fake_score=0.0234, time=28.33s\nEpoch 3/5: d_loss=0.0218, g_loss=7.1709, real_score=0.9895, fake_score=0.0131, time=28.36s\nEpoch 4/5: d_loss=0.0150, g_loss=7.9728, real_score=0.9929, fake_score=0.0087, time=28.26s\nEpoch 5/5: d_loss=0.0120, g_loss=8.6648, real_score=0.9945, fake_score=0.0067, time=28.44s\n\n### Activation: LeakyReLU\nEpoch 1/5: d_loss=0.1149, g_loss=4.9389, real_score=0.9425, fake_score=0.1112, time=28.23s\nEpoch 2/5: d_loss=0.0355, g_loss=6.6925, real_score=0.9826, fake_score=0.0229, time=28.28s\nEpoch 3/5: d_loss=0.0212, g_loss=7.3810, real_score=0.9898, fake_score=0.0127, time=28.27s\nEpoch 4/5: d_loss=0.0145, g_loss=8.2105, real_score=0.9933, fake_score=0.0082, time=28.52s\nEpoch 5/5: d_loss=0.0116, g_loss=9.6496, real_score=0.9950, fake_score=0.0062, time=27.93s\n\n### Activation: PReLU\nEpoch 1/5: d_loss=0.1467, g_loss=3.8452, real_score=0.9244, fake_score=0.1364, time=30.87s\nEpoch 2/5: d_loss=0.0335, g_loss=5.8214, real_score=0.9826, fake_score=0.0208, time=31.25s\nEpoch 3/5: d_loss=0.0197, g_loss=7.1753, real_score=0.9905, fake_score=0.0116, time=31.04s\nEpoch 4/5: d_loss=0.0132, g_loss=8.3105, real_score=0.9941, fake_score=0.0075, time=31.04s\nEpoch 5/5: d_loss=0.0090, g_loss=10.1721, real_score=0.9961, fake_score=0.0051, time=31.06s\n\n### Activation: SoftReLU\nEpoch 1/5: d_loss=0.0923, g_loss=4.2552, real_score=0.9664, fake_score=0.1003, time=29.70s\nEpoch 2/5: d_loss=0.0348, g_loss=5.8594, real_score=0.9829, fake_score=0.0187, time=29.73s\nEpoch 3/5: d_loss=0.0231, g_loss=6.3664, real_score=0.9890, fake_score=0.0114, time=29.87s\nEpoch 4/5: d_loss=0.0155, g_loss=6.8919, real_score=0.9930, fake_score=0.0075, time=30.05s\nEpoch 5/5: d_loss=0.0107, g_loss=7.4423, real_score=0.9952, fake_score=0.0049, time=29.98s\n\n### Activation: ESReLU\nEpoch 1/5: d_loss=0.1111, g_loss=4.5353, real_score=0.9423, fake_score=0.1078, time=35.11s\nEpoch 2/5: d_loss=0.0284, g_loss=6.2710, real_score=0.9852, fake_score=0.0183, time=35.62s\nEpoch 3/5: d_loss=0.0181, g_loss=7.3052, real_score=0.9913, fake_score=0.0101, time=35.08s\nEpoch 4/5: d_loss=0.0126, g_loss=8.3163, real_score=0.9942, fake_score=0.0065, time=34.90s\nEpoch 5/5: d_loss=0.0111, g_loss=9.0008, real_score=0.9951, fake_score=0.0054, time=34.90s\n\n### Activation: APU\nEpoch 1/5: d_loss=0.2867, g_loss=2.2356, real_score=0.8408, fake_score=0.2081, time=39.43s\nEpoch 2/5: d_loss=0.0773, g_loss=4.1892, real_score=0.9620, fake_score=0.0426, time=39.46s\nEpoch 3/5: d_loss=0.0293, g_loss=5.6914, real_score=0.9873, fake_score=0.0143, time=39.38s\nEpoch 4/5: d_loss=0.0212, g_loss=6.4725, real_score=0.9910, fake_score=0.0098, time=39.35s\nEpoch 5/5: d_loss=0.0148, g_loss=7.1423, real_score=0.9938, fake_score=0.0067, time=39.09s\nModels initialized for GAN on EMNIST: 12\nModels used for GAN on EMNIST: 12\n\n===== Gradient Flow Analysis =====\n\nAnalyzing gradient flow for SimpleCNN with ReLU on MNIST...\n  - Mean gradient: 0.252451\n  - Gradient std: 0.423008\n  - Vanishing gradient ratio: 12.98%\n\nAnalyzing gradient flow for SimpleCNN with LeakyReLU on MNIST...\n  - Mean gradient: 0.309381\n  - Gradient std: 0.549896\n  - Vanishing gradient ratio: 0.05%\n\nAnalyzing gradient flow for SimpleCNN with PReLU on MNIST...\n  - Mean gradient: 0.191346\n  - Gradient std: 0.409513\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for SimpleCNN with SoftReLU on MNIST...\n  - Mean gradient: 0.120396\n  - Gradient std: 0.187150\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for SimpleCNN with ESReLU on MNIST...\n  - Mean gradient: 0.247836\n  - Gradient std: 0.428959\n  - Vanishing gradient ratio: 0.07%\n\nAnalyzing gradient flow for SimpleCNN with APU on MNIST...\n  - Mean gradient: 0.265929\n  - Gradient std: 0.764981\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with ReLU on MNIST...\n  - Mean gradient: 0.661559\n  - Gradient std: 0.980646\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with LeakyReLU on MNIST...\n  - Mean gradient: 0.641890\n  - Gradient std: 0.960389\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with PReLU on MNIST...\n  - Mean gradient: 0.437023\n  - Gradient std: 0.747713\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with SoftReLU on MNIST...\n  - Mean gradient: 0.504445\n  - Gradient std: 0.766610\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with ESReLU on MNIST...\n  - Mean gradient: 0.594734\n  - Gradient std: 0.877942\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with APU on MNIST...\n  - Mean gradient: 0.294913\n  - Gradient std: 0.664794\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for Transformer with ReLU on MNIST...\n  - Mean gradient: 0.291484\n  - Gradient std: 0.460256\n  - Vanishing gradient ratio: 8.87%\n\nAnalyzing gradient flow for Transformer with LeakyReLU on MNIST...\n  - Mean gradient: 0.342025\n  - Gradient std: 0.481174\n  - Vanishing gradient ratio: 0.06%\n\nAnalyzing gradient flow for Transformer with PReLU on MNIST...\n  - Mean gradient: 0.399327\n  - Gradient std: 0.576177\n  - Vanishing gradient ratio: 0.05%\n\nAnalyzing gradient flow for Transformer with SoftReLU on MNIST...\n  - Mean gradient: 0.323967\n  - Gradient std: 0.494919\n  - Vanishing gradient ratio: 0.05%\n\nAnalyzing gradient flow for Transformer with ESReLU on MNIST...\n  - Mean gradient: 0.328210\n  - Gradient std: 0.484740\n  - Vanishing gradient ratio: 0.05%\n\nAnalyzing gradient flow for Transformer with APU on MNIST...\n  - Mean gradient: 0.350476\n  - Gradient std: 0.552735\n  - Vanishing gradient ratio: 0.05%\n\nAnalyzing gradient flow for SimpleCNN with ReLU on CIFAR10...\n  - Mean gradient: 0.239091\n  - Gradient std: 0.415030\n  - Vanishing gradient ratio: 6.58%\n\nAnalyzing gradient flow for SimpleCNN with LeakyReLU on CIFAR10...\n  - Mean gradient: 0.236661\n  - Gradient std: 0.419485\n  - Vanishing gradient ratio: 0.01%\n\nAnalyzing gradient flow for SimpleCNN with PReLU on CIFAR10...\n  - Mean gradient: 0.134763\n  - Gradient std: 0.290615\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for SimpleCNN with SoftReLU on CIFAR10...\n  - Mean gradient: 0.092598\n  - Gradient std: 0.141199\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for SimpleCNN with ESReLU on CIFAR10...\n  - Mean gradient: 0.201733\n  - Gradient std: 0.338142\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for SimpleCNN with APU on CIFAR10...\n  - Mean gradient: 0.225057\n  - Gradient std: 0.636479\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with ReLU on CIFAR10...\n  - Mean gradient: 0.712315\n  - Gradient std: 1.057597\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with LeakyReLU on CIFAR10...\n  - Mean gradient: 0.710263\n  - Gradient std: 1.034002\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with PReLU on CIFAR10...\n  - Mean gradient: 0.516034\n  - Gradient std: 0.876259\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with SoftReLU on CIFAR10...\n  - Mean gradient: 0.500489\n  - Gradient std: 0.731299\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with ESReLU on CIFAR10...\n  - Mean gradient: 0.713480\n  - Gradient std: 1.099240\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with APU on CIFAR10...\n  - Mean gradient: 0.480563\n  - Gradient std: 1.095415\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for Transformer with ReLU on CIFAR10...\n  - Mean gradient: 0.631961\n  - Gradient std: 0.953768\n  - Vanishing gradient ratio: 4.61%\n\nAnalyzing gradient flow for Transformer with LeakyReLU on CIFAR10...\n  - Mean gradient: 0.471005\n  - Gradient std: 0.870025\n  - Vanishing gradient ratio: 0.03%\n\nAnalyzing gradient flow for Transformer with PReLU on CIFAR10...\n  - Mean gradient: 0.440776\n  - Gradient std: 0.804371\n  - Vanishing gradient ratio: 0.03%\n\nAnalyzing gradient flow for Transformer with SoftReLU on CIFAR10...\n  - Mean gradient: 0.556000\n  - Gradient std: 0.934829\n  - Vanishing gradient ratio: 0.03%\n\nAnalyzing gradient flow for Transformer with ESReLU on CIFAR10...\n  - Mean gradient: 0.562216\n  - Gradient std: 0.868046\n  - Vanishing gradient ratio: 0.03%\n\nAnalyzing gradient flow for Transformer with APU on CIFAR10...\n  - Mean gradient: 0.449989\n  - Gradient std: 0.792120\n  - Vanishing gradient ratio: 0.03%\n\nAnalyzing gradient flow for SimpleCNN with ReLU on EMNIST...\n  - Mean gradient: 0.287134\n  - Gradient std: 0.507845\n  - Vanishing gradient ratio: 9.40%\n\nAnalyzing gradient flow for SimpleCNN with LeakyReLU on EMNIST...\n  - Mean gradient: 0.228045\n  - Gradient std: 0.385395\n  - Vanishing gradient ratio: 0.03%\n\nAnalyzing gradient flow for SimpleCNN with PReLU on EMNIST...\n  - Mean gradient: 0.172339\n  - Gradient std: 0.363784\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for SimpleCNN with SoftReLU on EMNIST...\n  - Mean gradient: 0.130807\n  - Gradient std: 0.202848\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for SimpleCNN with ESReLU on EMNIST...\n  - Mean gradient: 0.215305\n  - Gradient std: 0.365935\n  - Vanishing gradient ratio: 0.01%\n\nAnalyzing gradient flow for SimpleCNN with APU on EMNIST...\n  - Mean gradient: 0.287243\n  - Gradient std: 0.809458\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with ReLU on EMNIST...\n  - Mean gradient: 0.686156\n  - Gradient std: 1.049809\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with LeakyReLU on EMNIST...\n  - Mean gradient: 0.652033\n  - Gradient std: 0.974834\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with PReLU on EMNIST...\n  - Mean gradient: 0.448378\n  - Gradient std: 0.769230\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with SoftReLU on EMNIST...\n  - Mean gradient: 0.469280\n  - Gradient std: 0.687356\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with ESReLU on EMNIST...\n  - Mean gradient: 0.628445\n  - Gradient std: 0.939121\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for ResNet with APU on EMNIST...\n  - Mean gradient: 0.259821\n  - Gradient std: 0.583482\n  - Vanishing gradient ratio: 0.00%\n\nAnalyzing gradient flow for Transformer with ReLU on EMNIST...\n  - Mean gradient: 0.320455\n  - Gradient std: 0.475235\n  - Vanishing gradient ratio: 6.69%\n\nAnalyzing gradient flow for Transformer with LeakyReLU on EMNIST...\n  - Mean gradient: 0.352791\n  - Gradient std: 0.502504\n  - Vanishing gradient ratio: 0.06%\n\nAnalyzing gradient flow for Transformer with PReLU on EMNIST...\n  - Mean gradient: 0.288736\n  - Gradient std: 0.449059\n  - Vanishing gradient ratio: 0.05%\n\nAnalyzing gradient flow for Transformer with SoftReLU on EMNIST...\n  - Mean gradient: 0.343338\n  - Gradient std: 0.472102\n  - Vanishing gradient ratio: 0.05%\n\nAnalyzing gradient flow for Transformer with ESReLU on EMNIST...\n  - Mean gradient: 0.345375\n  - Gradient std: 0.486764\n  - Vanishing gradient ratio: 0.05%\n\nAnalyzing gradient flow for Transformer with APU on EMNIST...\n  - Mean gradient: 0.287179\n  - Gradient std: 0.464134\n  - Vanishing gradient ratio: 0.05%\n\n\n===== All benchmarks completed =====\nTotal models initialized: 78\nTotal models used: 78\nResults saved to 'results/' directory\n","output_type":"stream"}],"execution_count":28}]}